{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmarMlaeb/AAI612_Malaeb/blob/master/Week%204/Notebook4.4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7PsylxpEg2Z"
      },
      "source": [
        "\n",
        "\n",
        "# AAI612: Deep Learning & its Applications\n",
        "\n",
        "*Notebook 4.3: Graded Assignment: Mini Project I*\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/OmarMlaeb/AAI612_Malaeb/blob/master/Week%204/Notebook4.4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZKhtT1hEg2a"
      },
      "source": [
        "# Assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SWTmbAqEg2a"
      },
      "source": [
        "In this assessment, you will train a new model that is able to recognize fresh and rotten fruit. You will need to get the model to a validation accuracy of `92%` in order to pass the assessment, though we challenge you to do even better if you can. You will have the use the skills that you learned in the previous exercises. Specifically, we suggest using some combination of transfer learning, data augmentation, and fine tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j31mDC7HEg2a"
      },
      "source": [
        "## The Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9nGM-jwEg2a"
      },
      "source": [
        "In this exercise, you will train a model to recognize fresh and rotten fruits. Download the dataset from [Kaggle](https://www.kaggle.com/sriramr/fruits-fresh-and-rotten-for-classification). The dataset structure is in the `data/fruits` folder. There are 6 categories of fruits: fresh apples, fresh oranges, fresh bananas, rotten apples, rotten oranges, and rotten bananas. This will mean that your model will require an output layer of 6 neurons to do the categorization successfully. You'll also need to compile the model with `categorical_crossentropy`, as we have more than two categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efuZUPaVEg2a"
      },
      "source": [
        "![image.png](attachment:4c8c02c9-0cbe-4048-8d01-cdd5e3cf3fe6.png)<img src=\"./images/fruits.png\" style=\"width: 600px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KrjYu7MEg2a"
      },
      "source": [
        "## Load ImageNet Base Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRB8MbaCEg2a"
      },
      "source": [
        "Start with a model pretrained on `ImageNet`. Load the model with the correct weights, set an input shape, and choose to remove the last layers of the model. Remember that images have three dimensions: a height, and width, and a number of channels. Because these pictures are in color, there will be three channels for red, green, and blue. We've filled in the input shape for you. This cannot be changed or the assessment will fail. If you need a reference for setting up the pretrained model, please take a look at [Notebook 4.2](https://github.com/harmanani/AAI612/blob/main/Week4/Notebook%204.2.ipynb) where we implemented transfer learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWkuZ5Z_Eg2a"
      },
      "outputs": [],
      "source": [
        "import ssl\n",
        "from tensorflow import keras\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# loading VGG16 without the top layers\n",
        "base_model = keras.applications.VGG16(\n",
        "    weights='imagenet',\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False) # fixed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi3IILqUEg2b"
      },
      "source": [
        "## Freeze Base Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXIHmF9LEg2b"
      },
      "source": [
        "Next, we suggest freezing the base model. This is done so that all the learning from the ImageNet dataset does not get destroyed in the initial training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeackF1YEg2b"
      },
      "outputs": [],
      "source": [
        "# Freeze base model\n",
        "base_model.trainable = False # fixed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKiCBtr5Eg2b"
      },
      "source": [
        "## Add Layers to Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bqtoqq9Eg2b"
      },
      "source": [
        "Now it's time to add layers to the pretrained model. Pay close attention to the last dense layer and make sure it has the correct number of neurons to classify the different types of fruit.  You may add more layers than specified below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mvOlwCREg2b"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten\n",
        "\n",
        "# Create inputs with correct shape\n",
        "inputs = keras.Input(shape=(224, 224, 3)) # fixed\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "\n",
        "# Add pooling layer or flatten layer\n",
        "x = GlobalAveragePooling2D()(x)  # used global average pooling instead of flatten\n",
        "x = Dense(256, activation='relu')(x)  # added a dense hidden layer\n",
        "x = Dropout(0.5)(x)  # dropout to prevent overfitting\n",
        "\n",
        "# Add final dense layer\n",
        "outputs = keras.layers.Dense(6, activation='softmax')(x) # 6 output neurons (one per class)\n",
        "\n",
        "# Combine inputs and outputs to create model\n",
        "model = keras.Model(inputs=inputs, outputs=outputs) # fixed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WymqdGuEg2b",
        "outputId": "503106c7-cdd6-45bb-bd2e-f16bb808875a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 512)              0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               131328    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6)                 1542      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,847,558\n",
            "Trainable params: 132,870\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFXuCmrgEg2c"
      },
      "source": [
        "## Compile Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMajgLEnEg2c"
      },
      "source": [
        "Now it's time to compile the model with loss and metrics options. Remember that we're training on a number of different categories, rather than a binary classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSvudIofEg2c"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwRC9LHUEg2c"
      },
      "source": [
        "## Augment the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3VDmTh8Eg2c"
      },
      "source": [
        "If you'd like, try to augment the data to improve the dataset. There is also documentation for the [Keras ImageDataGenerator class](https://keras.io/api/preprocessing/image/#imagedatagenerator-class). This step is optional, but it may be helpful to get to 92% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1D0jeN9ZEg2c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2  # splitted dataset into 80% train, 20% validation\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyqYZLc1Eg2c"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KOpF19bEg2c"
      },
      "source": [
        "Now it's time to load the train and validation datasets. Pick the right folders, as well as the right `target_size` of the images (it needs to match the height and width input of the model you've created)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saKJ8USsEg2c",
        "outputId": "d47c5572-2312-41eb-fd4f-5e6c3e0ce927"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8723 images belonging to 6 classes.\n",
            "Found 2178 images belonging to 6 classes.\n"
          ]
        }
      ],
      "source": [
        "# Load and iterate training dataset\n",
        "train_it = datagen.flow_from_directory(\n",
        "    'data/fruits',\n",
        "    target_size=(224, 224),\n",
        "    color_mode='rgb',\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"training\"\n",
        ") # fixed\n",
        "\n",
        "# Load and iterate validation dataset\n",
        "valid_it = datagen.flow_from_directory(\n",
        "    'data/fruits',\n",
        "    target_size=(224, 224),\n",
        "    color_mode='rgb',\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"validation\"\n",
        ") # fixed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMLur5sjEg2c"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkR7pVXtEg2c"
      },
      "source": [
        "Time to train the model! Pass the `train` and `valid` iterators into the `fit` function, as well as setting your desired number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILgab6eCEg2c",
        "outputId": "f30486bb-5acf-489a-dcc6-ad656ddb19a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "272/272 [==============================] - 123s 382ms/step - loss: 0.9790 - accuracy: 0.6392 - val_loss: 0.5255 - val_accuracy: 0.8364\n",
            "Epoch 2/20\n",
            "272/272 [==============================] - 64s 235ms/step - loss: 0.5327 - accuracy: 0.8095 - val_loss: 0.3639 - val_accuracy: 0.8690\n",
            "Epoch 3/20\n",
            "272/272 [==============================] - 71s 261ms/step - loss: 0.4252 - accuracy: 0.8434 - val_loss: 0.2972 - val_accuracy: 0.8998\n",
            "Epoch 4/20\n",
            "272/272 [==============================] - 115s 424ms/step - loss: 0.3681 - accuracy: 0.8654 - val_loss: 0.2693 - val_accuracy: 0.9058\n",
            "Epoch 5/20\n",
            "272/272 [==============================] - 115s 424ms/step - loss: 0.3308 - accuracy: 0.8783 - val_loss: 0.2317 - val_accuracy: 0.9145\n",
            "Epoch 6/20\n",
            "272/272 [==============================] - 120s 440ms/step - loss: 0.3017 - accuracy: 0.8877 - val_loss: 0.1898 - val_accuracy: 0.9375\n",
            "Epoch 7/20\n",
            "272/272 [==============================] - 115s 421ms/step - loss: 0.2739 - accuracy: 0.9000 - val_loss: 0.1984 - val_accuracy: 0.9311\n",
            "Epoch 8/20\n",
            "272/272 [==============================] - 114s 421ms/step - loss: 0.2656 - accuracy: 0.9021 - val_loss: 0.1961 - val_accuracy: 0.9265\n",
            "Epoch 9/20\n",
            "272/272 [==============================] - 115s 421ms/step - loss: 0.2615 - accuracy: 0.9043 - val_loss: 0.1973 - val_accuracy: 0.9278\n",
            "Epoch 10/20\n",
            "272/272 [==============================] - 114s 420ms/step - loss: 0.2516 - accuracy: 0.9055 - val_loss: 0.1818 - val_accuracy: 0.9315\n",
            "Epoch 11/20\n",
            "272/272 [==============================] - 118s 432ms/step - loss: 0.2369 - accuracy: 0.9105 - val_loss: 0.1454 - val_accuracy: 0.9508\n",
            "Epoch 12/20\n",
            "272/272 [==============================] - 115s 424ms/step - loss: 0.2378 - accuracy: 0.9097 - val_loss: 0.1559 - val_accuracy: 0.9508\n",
            "Epoch 13/20\n",
            "272/272 [==============================] - 115s 422ms/step - loss: 0.2289 - accuracy: 0.9140 - val_loss: 0.1737 - val_accuracy: 0.9393\n",
            "Epoch 14/20\n",
            "272/272 [==============================] - 116s 425ms/step - loss: 0.2172 - accuracy: 0.9192 - val_loss: 0.1262 - val_accuracy: 0.9531\n",
            "Epoch 15/20\n",
            "272/272 [==============================] - 121s 446ms/step - loss: 0.2154 - accuracy: 0.9210 - val_loss: 0.1381 - val_accuracy: 0.9527\n",
            "Epoch 16/20\n",
            "272/272 [==============================] - 119s 439ms/step - loss: 0.2149 - accuracy: 0.9178 - val_loss: 0.1246 - val_accuracy: 0.9527\n",
            "Epoch 17/20\n",
            "272/272 [==============================] - 115s 424ms/step - loss: 0.2130 - accuracy: 0.9246 - val_loss: 0.1120 - val_accuracy: 0.9628\n",
            "Epoch 18/20\n",
            "272/272 [==============================] - 114s 421ms/step - loss: 0.2046 - accuracy: 0.9262 - val_loss: 0.1851 - val_accuracy: 0.9283\n",
            "Epoch 19/20\n",
            "272/272 [==============================] - 114s 420ms/step - loss: 0.2011 - accuracy: 0.9246 - val_loss: 0.1527 - val_accuracy: 0.9407\n",
            "Epoch 20/20\n",
            "272/272 [==============================] - 116s 428ms/step - loss: 0.1887 - accuracy: 0.9320 - val_loss: 0.1336 - val_accuracy: 0.9504\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1c303149b40>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(\n",
        "    train_it,\n",
        "    validation_data=valid_it,\n",
        "    steps_per_epoch=train_it.samples//train_it.batch_size,\n",
        "    validation_steps=valid_it.samples//valid_it.batch_size,\n",
        "    epochs=20\n",
        ") # fixed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TFWcS3kEg2c"
      },
      "source": [
        "## Unfreeze Model for Fine Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDy-dgCREg2c"
      },
      "source": [
        "If you have reached 92% validation accuracy already, this next step is optional. If not, we suggest fine tuning the model with a very low learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFsIemowEg2c"
      },
      "outputs": [],
      "source": [
        "# Unfreeze the base model\n",
        "base_model.trainable = True # fixed\n",
        "\n",
        "# Compile the model with a low learning rate\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy']) # fixed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOe49wTgEg2c",
        "outputId": "859cc819-ad56-4a55-f323-ba0b4ee7ba4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "272/272 [==============================] - 131s 439ms/step - loss: 0.2019 - accuracy: 0.9406 - val_loss: 0.0486 - val_accuracy: 0.9839\n",
            "Epoch 2/10\n",
            "272/272 [==============================] - 115s 424ms/step - loss: 0.0773 - accuracy: 0.9733 - val_loss: 0.0580 - val_accuracy: 0.9789\n",
            "Epoch 3/10\n",
            "272/272 [==============================] - 116s 425ms/step - loss: 0.0451 - accuracy: 0.9864 - val_loss: 0.0554 - val_accuracy: 0.9825\n",
            "Epoch 4/10\n",
            "272/272 [==============================] - 116s 424ms/step - loss: 0.0302 - accuracy: 0.9895 - val_loss: 0.0105 - val_accuracy: 0.9982\n",
            "Epoch 5/10\n",
            "272/272 [==============================] - 116s 424ms/step - loss: 0.0280 - accuracy: 0.9921 - val_loss: 0.0022 - val_accuracy: 0.9991\n",
            "Epoch 6/10\n",
            "272/272 [==============================] - 115s 423ms/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.0036 - val_accuracy: 0.9991\n",
            "Epoch 7/10\n",
            "272/272 [==============================] - 115s 423ms/step - loss: 0.0171 - accuracy: 0.9952 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "272/272 [==============================] - 115s 423ms/step - loss: 0.0110 - accuracy: 0.9962 - val_loss: 3.9696e-04 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "272/272 [==============================] - 115s 423ms/step - loss: 0.0159 - accuracy: 0.9953 - val_loss: 1.6365e-04 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "272/272 [==============================] - 115s 423ms/step - loss: 0.0126 - accuracy: 0.9970 - val_loss: 0.0041 - val_accuracy: 0.9986\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1c07db93700>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(\n",
        "    train_it,\n",
        "    validation_data=valid_it,\n",
        "    steps_per_epoch=train_it.samples//train_it.batch_size,\n",
        "    validation_steps=valid_it.samples//valid_it.batch_size,\n",
        "    epochs=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P1S5T8OEg2d"
      },
      "source": [
        "## Evaluate the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xwnf9KWgEg2d"
      },
      "source": [
        "Hopefully, you now have a model that has a validation accuracy of 92% or higher. If not, you may want to go back and either run more epochs of training, or adjust your data augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhIl9weHEg2d"
      },
      "source": [
        "## Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5knB_kEEg2d"
      },
      "source": [
        "In this project, we used transfer learning with the VGG16 model to classify images into six different categories. At first, we loaded the pre-trained VGG16 model without its fully connected layers and kept its ImageNet weights to take advantage of its previously learned features. To prevent overfitting, we froze the model’s parameters and added a Global Average Pooling layer instead of Flatten, followed by dense layers with ReLU activation and Dropout for regularization.\n",
        "\n",
        "To make the model more robust, we applied data augmentation techniques like rotation, zoom, and flipping using ImageDataGenerator. This helped improve generalization and prevent overfitting. The model was then compiled and trained for 20 epochs.\n",
        "\n",
        "Since the initial training results showed good results, we fine-tuned the model by unfreezing the base layers and training it again with a lower learning rate using the RMSprop optimizer. This step allowed the model to refine its learned features, leading to a significant increase in accuracy. By balancing feature extraction and fine-tuning, we achieved near-perfect validation accuracy, making this approach well-suited for small to medium-sized datasets.\n",
        "\n",
        "#### Phase 1: Initial Model Training\n",
        "\n",
        "During the initial training phase, the model showed consistent improvement in both training and validation accuracy. In which the first epoch started with 63.92% accuracy, and after 20 epochs, it reached 93.20% training accuracy and 95.04% validation accuracy. The model's loss gradually decreased, showing that it was learning effectively. However the validation accuracy fluctuated slightly in later epochs, indicating the need for fine-tuning.\n",
        "\n",
        "#### Phase 2: Fine-Tuning with Unfreezing and Lower Learning Rate\n",
        "\n",
        "To optimize performance I unfrozed the model layers to allow fine-tuning of deeper representations and lowered the learning rate to stabilize the training process. Showing that the results were significantly improved, the model reached 99.70% training accuracy and 99.86% validation accuracy after 10 epochs. The validation accuracy hit 100% during some epochs and the validation loss became extremely low, reaching near zero in later epochs.\n",
        "\n",
        "\n",
        "As we can see Fine-tuning increased performance, the accuracy jumped from 93.20% to 99.70%, indicating that unfreezing layers allowed the model to extract deeper features from the data. moreover, lowering the learning rate stabilized training in which it avoided abrupt weight updates and reduced the risk of overfitting. While the model achieved near-perfect accuracy, the very low validation loss suggests it may have memorized the training data."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}